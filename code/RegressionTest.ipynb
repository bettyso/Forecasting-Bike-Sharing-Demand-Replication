{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import 2013 Bike Sharing data\n",
      "   tractID  jday  season  mth  hr  holiday  weekday  workingday  weather  \\\n",
      "0        2     1       1    1   0        1        2           0        2   \n",
      "1        8     1       1    1   0        1        2           0        2   \n",
      "2       16     1       1    1   0        1        2           0        2   \n",
      "3       17     1       1    1   0        1        2           0        2   \n",
      "4       19     1       1    1   0        1        2           0        2   \n",
      "\n",
      "   temp  atemp  humidity  windspeed  casual  registered  cnt  mday  \n",
      "0   3.3    0.4        68       11.1       0           1    1     1  \n",
      "1   3.3    0.4        68       11.1       0           1    1     1  \n",
      "2   3.3    0.4        68       11.1       0           1    1     1  \n",
      "3   3.3    0.4        68       11.1       0           1    1     1  \n",
      "4   3.3    0.4        68       11.1       0           2    2     1  \n",
      "2017-11-25 22:07:24 - Start processing LinearRegression\n",
      "2017-11-25 22:07:29 - Start processing RidgeRegression for solver: auto\n",
      "2017-11-25 22:07:33 - Start processing RidgeRegression for solver: svd\n",
      "2017-11-25 22:07:40 - Start processing RidgeRegression for solver: lsqr\n",
      "2017-11-25 22:07:44 - Start processing RidgeRegression for solver: sparse_cg\n",
      "2017-11-25 22:07:47 - Start processing RidgeRegression for solver: saga\n",
      "2017-11-25 22:08:01 - Start processing BayesianRidge\n",
      "2017-11-25 22:08:09 - Start processing SGDRegressor\n",
      "    tractID  jday        hr  holiday  workingday  atemp  humidity  windspeed  \\\n",
      "0  0.008929     1 -2.245598        1           0    0.4  4.234107   2.493205   \n",
      "1  0.062500     1 -2.245598        1           0    0.4  4.234107   2.493205   \n",
      "2  0.133929     1 -2.245598        1           0    0.4  4.234107   2.493205   \n",
      "3  0.142857     1 -2.245598        1           0    0.4  4.234107   2.493205   \n",
      "4  0.160714     1 -2.245598        1           0    0.4  4.234107   2.493205   \n",
      "\n",
      "   casual  registered   ...    mth_2  mth_3  mth_4  mth_5  mth_6  mth_7  \\\n",
      "0     0.0    0.693147   ...      0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "1     0.0    0.693147   ...      0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "2     0.0    0.693147   ...      0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "3     0.0    0.693147   ...      0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "4     0.0    1.098612   ...      0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "\n",
      "   mth_8  mth_9  mth_10  mth_11  \n",
      "0    0.0    0.0     0.0     0.0  \n",
      "1    0.0    0.0     0.0     0.0  \n",
      "2    0.0    0.0     0.0     0.0  \n",
      "3    0.0    0.0     0.0     0.0  \n",
      "4    0.0    0.0     0.0     0.0  \n",
      "\n",
      "[5 rows x 41 columns]\n",
      "2017-11-25 22:13:09 - Start processing ExtraTreesRegressor for n_estimator:100  max_depth:20  max_features:sqrt\n",
      "2017-11-25 22:14:47 - Start processing ExtraTreesRegressor for n_estimator:100  max_depth:20  max_features:auto\n",
      "2017-11-25 22:23:09 - Start processing ExtraTreesRegressor for n_estimator:100  max_depth:25  max_features:sqrt\n",
      "2017-11-25 22:25:30 - Start processing ExtraTreesRegressor for n_estimator:100  max_depth:25  max_features:auto\n",
      "2017-11-25 22:35:25 - Start processing ExtraTreesRegressor for n_estimator:100  max_depth:30  max_features:sqrt\n",
      "2017-11-25 22:38:21 - Start processing ExtraTreesRegressor for n_estimator:100  max_depth:30  max_features:auto\n",
      "2017-11-25 22:49:58 - Start processing ExtraTreesRegressor for n_estimator:150  max_depth:20  max_features:sqrt\n",
      "2017-11-25 22:52:48 - Start processing ExtraTreesRegressor for n_estimator:150  max_depth:20  max_features:auto\n",
      "2017-11-25 23:07:56 - Start processing ExtraTreesRegressor for n_estimator:150  max_depth:25  max_features:sqrt\n",
      "2017-11-25 23:11:50 - Start processing ExtraTreesRegressor for n_estimator:150  max_depth:25  max_features:auto\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "import xgboost as xgb\n",
    "    \n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy import sparse \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from math import *\n",
    "\n",
    "# =====================================================================\n",
    "\n",
    "ifile = \"D:/Capston/2013_Hour_By_Tract.csv\"\n",
    "scores_cols = ['Test', 'fit_time', 'score_time', 'Test_score', 'Train_score']\n",
    "statfile = \"D:/Capston/RegStat_\" + dt.datetime.now().strftime(\"%Y%m%d%H%M%S\") + \".txt\"\n",
    "features=['tractID','mday','hr','tempCluster','hrCluster',\n",
    "          'season_0','season_1','season_2','season_3',\n",
    "          'weather_0','weather_1','weather_2','weather_3',\n",
    "          'weekday_0','weekday_1','weekday_2','weekday_3','weekday_4','weekday_5','weekday_6',\n",
    "          'mth_0','mth_1','mth_2','mth_3','mth_4','mth_5','mth_6','mth_7','mth_8','mth_9','mth_10','mth_11',\n",
    "          'holiday','workingday', 'windspeed','humidity']\n",
    "n_job=4\n",
    "score='neg_mean_squared_error'\n",
    "\n",
    "# =====================================================================\n",
    "\n",
    "def read_data():\n",
    "    ds = pd.read_csv(ifile, sep=',', header=0)\n",
    "    ds['dteday'] = pd.to_datetime(ds['dteday'], format='%Y-%m-%d')\n",
    "    ds['mday'] = ds['dteday'].dt.day \n",
    "    \n",
    "    return remove_columns(ds, ['dteday', 'yr'])\n",
    "\n",
    "\n",
    "def remove_columns(ds, drop_cols):\n",
    "    ds = ds.drop(drop_cols, axis = 1)\n",
    "\n",
    "    return ds\n",
    "\n",
    "# =====================================================================\n",
    "\n",
    "def store_scores(scores, TestScores, testname):\n",
    "    df = pd.DataFrame([[testname,\n",
    "                        scores['fit_time'].mean(),\n",
    "                        scores['score_time'].mean(),\n",
    "                        (-1 * scores['test_score'].mean()),\n",
    "                        (-1 * scores['train_score'].mean())]],\n",
    "                      columns = scores_cols)\n",
    "\n",
    "    df.to_csv(statfile, mode='a', header = False, index = False, sep = '|')\n",
    "    TestScores = TestScores.append(df, ignore_index = True)\n",
    "    #print (TestScores)\n",
    "    return TestScores\n",
    "\n",
    "# =====================================================================\n",
    "\n",
    "def split_list(alist, wanted_parts = 1):\n",
    "    length = len(alist)\n",
    "    return [ alist[i * length // wanted_parts: (i + 1) * length // wanted_parts]\n",
    "            for i in range(wanted_parts) ]\n",
    "\n",
    "\n",
    "def data_cluster(df, grpBy, cluster_num):\n",
    "    \n",
    "    cluster_data = df.groupby([grpBy]).agg(lambda x: x.mean())[['cnt']]\n",
    "    model = cluster.KMeans(n_clusters = cluster_num)\n",
    "    \n",
    "    return np.array(model.fit_predict(split_list(cluster_data.iloc[:,0].values, len(cluster_data))))\n",
    "\n",
    "\n",
    "def temp_cluster(temp):\n",
    "\n",
    "    if temp <= 1.0: \n",
    "        return 0\n",
    "    elif temp > 1.0 and temp <= 15.0:\n",
    "        return 1\n",
    "    elif temp > 15.0 and temp <= 22.0:\n",
    "        return 2\n",
    "    elif temp > 22.0 and temp <= 31.0:\n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n",
    "\n",
    "\n",
    "def hr_cluster(hr):\n",
    "\n",
    "    if hr <= 6.5: \n",
    "        return 0\n",
    "    elif hr > 6.5 and hr <= 9.5:\n",
    "        return 2\n",
    "    elif hr > 9.5 and hr <= 16.5:\n",
    "        return 1\n",
    "    elif hr > 16.5 and hr <= 20:\n",
    "        return 2\n",
    "    else:\n",
    "        return 0\n",
    "     \n",
    "\n",
    "def transform1_data(ds):\n",
    "    \n",
    "    ds['hrCluster'] = ds.apply(lambda x: hr_cluster(x['hr']), axis = 1) \n",
    "    ds['tempCluster'] = ds.apply(lambda x: temp_cluster(x['temp']), axis = 1) \n",
    "\n",
    "    enc = OneHotEncoder(sparse=False)  \n",
    "    n = ds['season'].shape[0]\n",
    "    enc_array = pd.DataFrame(sparse.csr_matrix(enc.fit_transform(ds['season'].values.reshape(-1, 1))).todense().reshape(n, 1, 4))\n",
    "    ds['season_0'] = enc_array[[0]]\n",
    "    ds['season_1'] = enc_array[[1]] \n",
    "    ds['season_2'] = enc_array[[2]] \n",
    "    ds['season_3'] = enc_array[[3]]     \n",
    "\n",
    "    enc_array = pd.DataFrame(sparse.csr_matrix(enc.fit_transform(ds['weather'].values.reshape(-1, 1))).todense().reshape(n, 1, 4))\n",
    "    ds['weather_0'] = enc_array[[0]]\n",
    "    ds['weather_1'] = enc_array[[1]] \n",
    "    ds['weather_2'] = enc_array[[2]] \n",
    "    ds['weather_3'] = enc_array[[3]]     \n",
    "\n",
    "    enc_array = pd.DataFrame(sparse.csr_matrix(enc.fit_transform(ds['weekday'].values.reshape(-1, 1))).todense().reshape(n, 1, 7))\n",
    "    ds['weekday_0'] = enc_array[[0]]\n",
    "    ds['weekday_1'] = enc_array[[1]] \n",
    "    ds['weekday_2'] = enc_array[[2]] \n",
    "    ds['weekday_3'] = enc_array[[3]]     \n",
    "    ds['weekday_4'] = enc_array[[4]]\n",
    "    ds['weekday_5'] = enc_array[[5]] \n",
    "    ds['weekday_6'] = enc_array[[6]] \n",
    "    \n",
    "    enc_array = pd.DataFrame(sparse.csr_matrix(enc.fit_transform(ds['mth'].values.reshape(-1, 1))).todense().reshape(n, 1, 12))\n",
    "    ds['mth_0'] = enc_array[[0]]\n",
    "    ds['mth_1'] = enc_array[[1]] \n",
    "    ds['mth_2'] = enc_array[[2]] \n",
    "    ds['mth_3'] = enc_array[[3]]     \n",
    "    ds['mth_4'] = enc_array[[4]]\n",
    "    ds['mth_5'] = enc_array[[5]] \n",
    "    ds['mth_6'] = enc_array[[6]] \n",
    "    ds['mth_7'] = enc_array[[7]]\n",
    "    ds['mth_8'] = enc_array[[8]] \n",
    "    ds['mth_9'] = enc_array[[9]] \n",
    "    ds['mth_10'] = enc_array[[10]]     \n",
    "    ds['mth_11'] = enc_array[[11]]\n",
    "\n",
    "    return remove_columns(ds, ['season', 'weather', 'weekday', 'mth', 'temp'])\n",
    "\n",
    "        \n",
    "def transform2_data(df):\n",
    "        \n",
    "    df['casual'] = [log1p(x) for x in df['casual']]\n",
    "    df['registered'] = [log1p(x) for x in df['registered']]\n",
    "    df['cnt'] = [log1p(x) for x in df['cnt']]\n",
    "\n",
    "    df['tractID'] = MinMaxScaler().fit_transform(df['tractID'].astype(float).values.reshape(-1, 1))\n",
    "    df['mday'] = StandardScaler().fit_transform(df['mday'].astype(float).values.reshape(-1, 1))\n",
    "    df['hr'] = StandardScaler().fit_transform(df['hr'].astype(float).values.reshape(-1, 1))\n",
    "    #df['weekday'] = StandardScaler().fit_transform(df['weekday'].astype(float).values.reshape(-1, 1))\n",
    "    #df['weather'] = StandardScaler().fit_transform(df['weather'].astype(float).values.reshape(-1, 1))\n",
    "    #df['temp'] = StandardScaler().fit_transform(df['temp'].values.reshape(-1, 1))\n",
    "    #df['jday'] = StandardScaler().fit_transform(df['jday'].astype(float).values.reshape(-1, 1))\n",
    "    #df['mth'] = StandardScaler().fit_transform(df['mth'].astype(float).values.reshape(-1, 1))\n",
    "    #df['season'] = StandardScaler().fit_transform(df['season'].astype(float).values.reshape(-1, 1))    \n",
    "    df['windspeed'] = [log1p(x) for x in df['windspeed']]\n",
    "    df['humidity'] = [log1p(x) for x in df['humidity']]\n",
    "    \n",
    "\n",
    "    print(df.head())\n",
    "    return df\n",
    "\n",
    "# =====================================================================\n",
    "\n",
    "def linearregression(X_data, y_data, TestScores):\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    \n",
    "    print(dt.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \n",
    "          \" - Start processing LinearRegression\")\n",
    "    model = LinearRegression(normalize = True)\n",
    "    scores = cross_validate(model, X_data, y_data, cv=10, \n",
    "                            scoring=score,\n",
    "                            return_train_score=True, n_jobs=n_job)\n",
    "    TestScores = store_scores(scores, TestScores,  \n",
    "                                \"LinearRegression\")\n",
    "\n",
    "    return TestScores\n",
    "\n",
    "# =====================================================================\n",
    "\n",
    "def ridgeregression(X_data, y_data, TestScores):\n",
    "    from sklearn.linear_model import Ridge\n",
    "    \n",
    "    for sv in ['auto', 'svd', 'lsqr', 'sparse_cg', 'saga']:\n",
    "        print(dt.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \n",
    "              \" - Start processing RidgeRegression for solver: %s\" %(sv))\n",
    "        model = Ridge(solver=sv, normalize = True, random_state=212)\n",
    "        scores = cross_validate(model, X_data, y_data, cv=10, \n",
    "                                scoring=score,\n",
    "                                return_train_score=True, n_jobs=n_job)\n",
    "        TestScores = store_scores(scores, TestScores,  \n",
    "                                \"RidgeRegression %s\" %(sv))\n",
    "\n",
    "    return TestScores\n",
    "\n",
    "# =====================================================================\n",
    "\n",
    "def bayesianridge(X_data, y_data, TestScores):\n",
    "    from sklearn.linear_model import BayesianRidge\n",
    "        \n",
    "    print(dt.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \n",
    "            \" - Start processing BayesianRidge\")\n",
    "    model = BayesianRidge(n_iter=1000, normalize = True)\n",
    "    scores = cross_validate(model, X_data, y_data, cv=10, \n",
    "                            scoring=score,\n",
    "                            return_train_score=True, n_jobs=n_job)\n",
    "    TestScores = store_scores(scores, TestScores,  \n",
    "                            \"BayesianRidge\")\n",
    "\n",
    "    return TestScores\n",
    "\n",
    "          \n",
    "# =====================================================================\n",
    "\n",
    "def sgdregressor(X_data, y_data, TestScores):\n",
    "    from sklearn.linear_model import SGDRegressor\n",
    "    \n",
    "    print(dt.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \n",
    "            \" - Start processing SGDRegressor\")\n",
    "    model = SGDRegressor(n_iter=1000, random_state=212)\n",
    "    scores = cross_validate(model, X_data, y_data, cv=10, \n",
    "                            scoring=score,\n",
    "                            return_train_score=True, n_jobs=n_job)\n",
    "    TestScores = store_scores(scores, TestScores,  \n",
    "                            \"SGDRegressor\")\n",
    "\n",
    "    return TestScores\n",
    "          \n",
    "          \n",
    "# =====================================================================\n",
    "\n",
    "def gradientboostingregression(X_data, y_data, TestScores):\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "#    for ne in [50, 100, 150, 200, 250, 300, 350, 400]:\n",
    "    for ne in [300, 350, 400]:\n",
    "        for md in [30, 35, 40]:\n",
    "            print(dt.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \n",
    "                    \" - Start processing GradientBoostingRegressor for n_estimator:%s  max_depth:%s\" %(ne, md))\n",
    "            model = GradientBoostingRegressor(n_estimators = ne, max_depth = md,\n",
    "                                            learning_rate = 0.1)\n",
    "            scores = cross_validate(model, X_data, y_data, cv=10, \n",
    "                                    scoring=score,\n",
    "                                    return_train_score=True, n_jobs=n_job)\n",
    "            TestScores = store_scores(scores, TestScores,  \n",
    "                                    \"GradientBoostingRegressor %s-%s\" %(ne, md))\n",
    "\n",
    "    return TestScores\n",
    "\n",
    "# =====================================================================\n",
    "\n",
    "def randomforestregression(X_data, y_data, TestScores):\n",
    "\n",
    "    for ne in [300, 400,500]:\n",
    "        for md in [30, 35, 40]:\n",
    "            print(dt.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \n",
    "                    \" - Start processing RandomForestRegressor for n_estimator:%s  max_depth:%s\" %(ne, md))\n",
    "            model = RandomForestRegressor(n_estimators = ne, max_depth = md, \n",
    "                                        max_features = 'auto', random_state = 120)\n",
    "            scores = cross_validate(model, X_data, y_data, cv=10, \n",
    "                                    scoring=score, \n",
    "                                    return_train_score=True, n_jobs=n_job)\n",
    "            TestScores = store_scores(scores, TestScores,  \n",
    "                                    \"RandomForestRegressor %s-%s\" %(ne, md))\n",
    "\n",
    "    return TestScores\n",
    "\n",
    "# =====================================================================\n",
    "\n",
    "def extratreesregression(X_data, y_data, TestScores):\n",
    "\n",
    "    for ne in [100, 150, 200]:\n",
    "        for md in [20, 25, 30]:\n",
    "            for mf in ['sqrt', 'auto']:\n",
    "                print(dt.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \n",
    "                      \" - Start processing ExtraTreesRegressor for n_estimator:%s  max_depth:%s  max_features:%s\" %(ne, md, mf))\n",
    "                model = ExtraTreesRegressor(n_estimators = ne, max_depth = md, \n",
    "                                            max_features = mf, random_state = 120)\n",
    "                scores = cross_validate(model, X_data, y_data, cv=10, \n",
    "                                        scoring=score, \n",
    "                                        return_train_score=True, n_jobs=n_job)\n",
    "                TestScores = store_scores(scores, TestScores,  \n",
    "                                        \"ExtraTreesRegressor %s-%s-%s\" %(ne, md, mf))\n",
    "\n",
    "    return TestScores\n",
    "\n",
    "# =====================================================================\n",
    "\n",
    "def baggingregression(X_data, y_data, TestScores):\n",
    "    from sklearn.ensemble import BaggingRegressor\n",
    "    \n",
    "    for ne in [100, 150, 200]:\n",
    "        print(dt.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \n",
    "                      \" - Start processing BaggingRegressor for n_estimator:%s\" %(ne))\n",
    "        model = BaggingRegressor(n_estimators = ne, bootstrap = True,\n",
    "                               bootstrap_features = True, random_state = 120)\n",
    "        scores = cross_validate(model, X_data, y_data, cv=10, \n",
    "                                scoring=score, \n",
    "                                return_train_score=True, n_jobs=n_job)\n",
    "        TestScores = store_scores(scores, TestScores, \"BaggingRegressor %s\" %(ne))\n",
    "\n",
    "    return TestScores\n",
    "\n",
    "# =====================================================================\n",
    "\n",
    "def xgboost(X_data, y_data, TestScores):\n",
    "    \n",
    "    for ne in [200, 250, 300]:\n",
    "        for lr in [0.05, 0.1]:\n",
    "            for bt in ['gblinear', 'dart']:\n",
    "                print(dt.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \n",
    "                        \" - Start processing XGBRegressor for n_estimator: %s learning-rate: %s booster: %s\" %(ne, lr, bt))\n",
    "                model = xgb.XGBRegressor(n_estimators = ne, learning_rate=lr,\n",
    "                                         booster=bt, random_state = 120)\n",
    "                scores = cross_validate(model, X_data, y_data, cv=10, \n",
    "                                        scoring=score, \n",
    "                                        return_train_score=True, n_jobs=n_job)\n",
    "                TestScores = store_scores(scores, TestScores, \"XGBRegressor %s-%s-%s\" %(ne, lr, bt))\n",
    "\n",
    "    return TestScores\n",
    "\n",
    "# =====================================================================\n",
    "\n",
    "def stacking(X_data, y_data):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import mean_squared_log_error    \n",
    "    from vecstack import stacking\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size = 0.2, random_state = 212)\n",
    "\n",
    "    mXgb = xgb.XGBRegressor(n_estimators = 50, learning_rate=0.05,booster='dart', seed = 0)\n",
    "    mEtr = ExtraTreesRegressor(n_estimators = 200, max_depth = 35,max_features = 'auto', random_state = 0)\n",
    "    mReg = RandomForestRegressor(n_estimators = 200, max_depth = 35,max_features = 'auto', random_state = 120)\n",
    "    models = [mXgb, mReg, mEtr]\n",
    "\n",
    "    S_train, S_test = stacking(models, X_train, y_train, X_test, regression = True,\n",
    "                               metric=mean_squared_log_error, n_folds = 5)\n",
    "    model = xgb.XGBRegressor(seed = 0, j_jobs = 2, learning_rate = 0.1, n_estimators = 200, max_depth = 35)\n",
    "\n",
    "    model = model.fit(S_train, y_train)\n",
    "    y_pred = model.predict(S_test)\n",
    "    \n",
    "    print ('Final prediction score: [%.8f]' % mean_squared_log_error(y_test, y_pred))\n",
    "\n",
    "# =====================================================================\n",
    "\n",
    "print(\"Import 2013 Bike Sharing data\")\n",
    "bike = read_data()\n",
    "TestScores = pd.DataFrame(columns = scores_cols)\n",
    "\n",
    "print(bike.head())\n",
    "\n",
    "#X = bike[['tractID','workingday','humidity','hr', 'temp','mthCluster','hrCluster','weather','weekday','mth','mday','windspeed','tempCluster','season_0','season_1','season_2','season_3']]\n",
    "y = bike[['cnt']]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    bike = transform1_data(bike)\n",
    "    X = bike[features]\n",
    "    linearregression(X, y, TestScores)\n",
    "    ridgeregression(X, y, TestScores)\n",
    "    bayesianridge(X, y.values.ravel(), TestScores)\n",
    "    sgdregressor(X, y.values.ravel(), TestScores)\n",
    "        \n",
    "    bike = transform2_data(bike)\n",
    "    X = bike[features]\n",
    "    #stacking(X.values, y.values.ravel())\n",
    "    #randomforestregression(X, y.values.ravel(), TestScores)\n",
    "    extratreesregression(X, y.values.ravel(), TestScores)\n",
    "    baggingregression(X, y.values.ravel(), TestScores)\n",
    "    xgboost(X, y.values.ravel(), TestScores)\n",
    "    gradientboostingregression(X, y.values.ravel(), TestScores)\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
